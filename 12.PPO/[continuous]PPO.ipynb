{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5bDijFjS7n9"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm8CH6jGM5hc",
        "outputId": "b1d5166d-ece1-4323-e7ef-b6858bb4c083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25henv: MUJOCO_GL=egl\n"
          ]
        }
      ],
      "source": [
        "!pip install -q numpy\n",
        "!pip install -q matplotlib\n",
        "!pip install -q mujoco\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "\n",
        "%env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGrpzOD_NHMH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "import mujoco\n",
        "\n",
        "import scipy.signal\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions.normal import Normal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTxEN97QTBVu"
      },
      "source": [
        "### Define MuJoCo Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8YPelbRNHtL"
      },
      "outputs": [],
      "source": [
        "xml_string=\"\"\"<!-- Cheetah Model\n",
        "\n",
        "    The state space is populated with joints in the order that they are\n",
        "    defined in this file. The actuators also operate on joints.\n",
        "\n",
        "    State-Space (name/joint/parameter):\n",
        "        - rootx     slider      position (m)\n",
        "        - rootz     slider      position (m)\n",
        "        - rooty     hinge       angle (rad)\n",
        "        - bthigh    hinge       angle (rad)\n",
        "        - bshin     hinge       angle (rad)\n",
        "        - bfoot     hinge       angle (rad)\n",
        "        - fthigh    hinge       angle (rad)\n",
        "        - fshin     hinge       angle (rad)\n",
        "        - ffoot     hinge       angle (rad)\n",
        "        - rootx     slider      velocity (m/s)\n",
        "        - rootz     slider      velocity (m/s)\n",
        "        - rooty     hinge       angular velocity (rad/s)\n",
        "        - bthigh    hinge       angular velocity (rad/s)\n",
        "        - bshin     hinge       angular velocity (rad/s)\n",
        "        - bfoot     hinge       angular velocity (rad/s)\n",
        "        - fthigh    hinge       angular velocity (rad/s)\n",
        "        - fshin     hinge       angular velocity (rad/s)\n",
        "        - ffoot     hinge       angular velocity (rad/s)\n",
        "\n",
        "    Actuators (name/actuator/parameter):\n",
        "        - bthigh    hinge       torque (N m)\n",
        "        - bshin     hinge       torque (N m)\n",
        "        - bfoot     hinge       torque (N m)\n",
        "        - fthigh    hinge       torque (N m)\n",
        "        - fshin     hinge       torque (N m)\n",
        "        - ffoot     hinge       torque (N m)\n",
        "\n",
        "-->\n",
        "<mujoco model=\"cheetah\">\n",
        "  <compiler angle=\"radian\" coordinate=\"local\" inertiafromgeom=\"true\" settotalmass=\"14\"/>\n",
        "  <default>\n",
        "    <joint armature=\".1\" damping=\".01\" limited=\"true\" solimplimit=\"0 .8 .03\" solreflimit=\".02 1\" stiffness=\"8\"/>\n",
        "    <geom conaffinity=\"0\" condim=\"3\" contype=\"1\" friction=\".4 .1 .1\" rgba=\"0.8 0.6 .4 1\" solimp=\"0.0 0.8 0.01\" solref=\"0.02 1\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1 1\"/>\n",
        "  </default>\n",
        "  <size nstack=\"300000\" nuser_geom=\"1\"/>\n",
        "  <option gravity=\"0 0 -9.81\" timestep=\"0.01\"/>\n",
        "  <asset>\n",
        "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"1 1 1\" rgb2=\"0 0 0\" type=\"skybox\" width=\"100\"/>\n",
        "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
        "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0 0 0\" rgb2=\"0.8 0.8 0.8\" type=\"2d\" width=\"100\"/>\n",
        "    <material name=\"MatPlane\" reflectance=\"0.5\" shininess=\"1\" specular=\"1\" texrepeat=\"60 60\" texture=\"texplane\"/>\n",
        "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
        "  </asset>\n",
        "  <worldbody>\n",
        "    <light cutoff=\"100\" diffuse=\"1 1 1\" dir=\"-0 0 -1.3\" directional=\"true\" exponent=\"1\" pos=\"0 0 1.3\" specular=\".1 .1 .1\"/>\n",
        "    <geom conaffinity=\"1\" condim=\"3\" material=\"MatPlane\" name=\"floor\" pos=\"0 0 0\" rgba=\"0.8 0.9 0.8 1\" size=\"40 40 40\" type=\"plane\"/>\n",
        "    <body name=\"torso\" pos=\"0 0 .7\">\n",
        "      <camera name=\"track\" mode=\"trackcom\" pos=\"0 -3 0.3\" xyaxes=\"1 0 0 0 0 1\"/>\n",
        "      <joint armature=\"0\" axis=\"1 0 0\" damping=\"0\" limited=\"false\" name=\"rootx\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
        "      <joint armature=\"0\" axis=\"0 0 1\" damping=\"0\" limited=\"false\" name=\"rootz\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
        "      <joint armature=\"0\" axis=\"0 1 0\" damping=\"0\" limited=\"false\" name=\"rooty\" pos=\"0 0 0\" stiffness=\"0\" type=\"hinge\"/>\n",
        "      <geom fromto=\"-.5 0 0 .5 0 0\" name=\"torso\" size=\"0.046\" type=\"capsule\"/>\n",
        "      <geom axisangle=\"0 1 0 .87\" name=\"head\" pos=\".6 0 .1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
        "      <!-- <site name='tip'  pos='.15 0 .11'/>-->\n",
        "      <body name=\"bthigh\" pos=\"-.5 0 0\">\n",
        "        <joint axis=\"0 1 0\" damping=\"6\" name=\"bthigh\" pos=\"0 0 0\" range=\"-.52 1.05\" stiffness=\"240\" type=\"hinge\"/>\n",
        "        <geom axisangle=\"0 1 0 -3.8\" name=\"bthigh\" pos=\".1 0 -.13\" size=\"0.046 .145\" type=\"capsule\"/>\n",
        "        <body name=\"bshin\" pos=\".16 0 -.25\">\n",
        "          <joint axis=\"0 1 0\" damping=\"4.5\" name=\"bshin\" pos=\"0 0 0\" range=\"-.785 .785\" stiffness=\"180\" type=\"hinge\"/>\n",
        "          <geom axisangle=\"0 1 0 -2.03\" name=\"bshin\" pos=\"-.14 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
        "          <body name=\"bfoot\" pos=\"-.28 0 -.14\">\n",
        "            <joint axis=\"0 1 0\" damping=\"3\" name=\"bfoot\" pos=\"0 0 0\" range=\"-.4 .785\" stiffness=\"120\" type=\"hinge\"/>\n",
        "            <geom axisangle=\"0 1 0 -.27\" name=\"bfoot\" pos=\".03 0 -.097\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .094\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"fthigh\" pos=\".5 0 0\">\n",
        "        <joint axis=\"0 1 0\" damping=\"4.5\" name=\"fthigh\" pos=\"0 0 0\" range=\"-1 .7\" stiffness=\"180\" type=\"hinge\"/>\n",
        "        <geom axisangle=\"0 1 0 .52\" name=\"fthigh\" pos=\"-.07 0 -.12\" size=\"0.046 .133\" type=\"capsule\"/>\n",
        "        <body name=\"fshin\" pos=\"-.14 0 -.24\">\n",
        "          <joint axis=\"0 1 0\" damping=\"3\" name=\"fshin\" pos=\"0 0 0\" range=\"-1.2 .87\" stiffness=\"120\" type=\"hinge\"/>\n",
        "          <geom axisangle=\"0 1 0 -.6\" name=\"fshin\" pos=\".065 0 -.09\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .106\" type=\"capsule\"/>\n",
        "          <body name=\"ffoot\" pos=\".13 0 -.18\">\n",
        "            <joint axis=\"0 1 0\" damping=\"1.5\" name=\"ffoot\" pos=\"0 0 0\" range=\"-.5 .5\" stiffness=\"60\" type=\"hinge\"/>\n",
        "            <geom axisangle=\"0 1 0 -.6\" name=\"ffoot\" pos=\".045 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .07\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor gear=\"120\" joint=\"bthigh\" name=\"bthigh\"/>\n",
        "    <motor gear=\"90\" joint=\"bshin\" name=\"bshin\"/>\n",
        "    <motor gear=\"60\" joint=\"bfoot\" name=\"bfoot\"/>\n",
        "    <motor gear=\"120\" joint=\"fthigh\" name=\"fthigh\"/>\n",
        "    <motor gear=\"60\" joint=\"fshin\" name=\"fshin\"/>\n",
        "    <motor gear=\"30\" joint=\"ffoot\" name=\"ffoot\"/>\n",
        "  </actuator>\n",
        "</mujoco>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhKDYlEkQo6X"
      },
      "outputs": [],
      "source": [
        "class HalfCheetahEnv():\n",
        "  def __init__(\n",
        "      self,\n",
        "      frame_skip=5,\n",
        "      forward_reward_weight=1.0,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      reset_noise_scale=0.1\n",
        "      ):\n",
        "\n",
        "    self.frame_skip = frame_skip\n",
        "    self.forward_reward_weight = forward_reward_weight\n",
        "    self.ctrl_cost_weight = ctrl_cost_weight\n",
        "    self.reset_noise_scale = reset_noise_scale\n",
        "\n",
        "    self.initialize_simulation()\n",
        "    self.init_qpos = self.data.qpos.ravel().copy()\n",
        "    self.init_qvel = self.data.qvel.ravel().copy()\n",
        "    self.dt = self.model.opt.timestep * self.frame_skip\n",
        "\n",
        "    self.observation_dim = 17\n",
        "    self.action_dim = 6\n",
        "    self.action_limit = 1.\n",
        "\n",
        "  def initialize_simulation(self):\n",
        "    self.model = mujoco.MjModel.from_xml_string(xml_string)\n",
        "    self.data = mujoco.MjData(self.model)\n",
        "    mujoco.mj_resetData(self.model, self.data)\n",
        "    self.renderer = mujoco.Renderer(self.model)\n",
        "\n",
        "  def reset_simulation(self):\n",
        "    mujoco.mj_resetData(self.model, self.data)\n",
        "\n",
        "  def step_mujoco_simulation(self, ctrl, n_frames):\n",
        "    self.data.ctrl[:] = ctrl\n",
        "    mujoco.mj_step(self.model, self.data, nstep=n_frames)\n",
        "    self.renderer.update_scene(self.data,0)\n",
        "\n",
        "  def set_state(self, qpos, qvel):\n",
        "    self.data.qpos[:] = np.copy(qpos)\n",
        "    self.data.qvel[:] = np.copy(qvel)\n",
        "    if self.model.na == 0:\n",
        "      self.data.act[:] = None\n",
        "    mujoco.mj_forward(self.model, self.data)\n",
        "\n",
        "  def sample_action(self):\n",
        "    return (2.*np.random.uniform(size=(self.action_dim,)) - 1)*self.action_limit\n",
        "\n",
        "  def step(self, action):\n",
        "    x_position_before = self.data.qpos[0]\n",
        "    self.step_mujoco_simulation(action, self.frame_skip)\n",
        "    x_position_after = self.data.qpos[0]\n",
        "    x_velocity = (x_position_after - x_position_before) / self.dt\n",
        "\n",
        "    # Rewards\n",
        "    ctrl_cost = self.ctrl_cost_weight * np.sum(np.square(action))\n",
        "    forward_reward = self.forward_reward_weight * x_velocity\n",
        "    observation = self.get_obs()\n",
        "    reward = forward_reward - ctrl_cost\n",
        "    terminated = False\n",
        "    info = {\n",
        "        \"x_position\": x_position_after,\n",
        "        \"x_velocity\": x_velocity,\n",
        "        \"reward_run\": forward_reward,\n",
        "        \"reward_ctrl\": -ctrl_cost,\n",
        "    }\n",
        "    return observation, reward, terminated, info\n",
        "\n",
        "  def get_obs(self):\n",
        "    position = self.data.qpos.flat.copy()\n",
        "    velocity = self.data.qvel.flat.copy()\n",
        "    position = position[1:]\n",
        "\n",
        "    observation = np.concatenate((position, velocity)).ravel()\n",
        "    return observation\n",
        "\n",
        "  def render(self):\n",
        "    return self.renderer.render()\n",
        "\n",
        "  def reset(self):\n",
        "    self.reset_simulation()\n",
        "    noise_low = -self.reset_noise_scale\n",
        "    noise_high = self.reset_noise_scale\n",
        "    qpos = self.init_qpos + np.random.uniform(\n",
        "        low=noise_low, high=noise_high, size=self.model.nq\n",
        "    )\n",
        "    qvel = (\n",
        "        self.init_qvel\n",
        "        + self.reset_noise_scale * np.random.standard_normal(self.model.nv)\n",
        "    )\n",
        "    self.set_state(qpos, qvel)\n",
        "    observation = self.get_obs()\n",
        "    return observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N59yQHJuqCvG",
        "outputId": "d44d4ea7-62f0-446c-9829-adf8bd3b65a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to : cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_ghjXIwTNvK"
      },
      "source": [
        "# Proximal Policy Optimization\n",
        "\n",
        "### Buffer 클래스: Buffer 클래스는 에피소드를 저장 하고 가공하는 역할\n",
        "\n",
        "- 저장 해야 할 것들\n",
        "  - state, $S_{t}$\n",
        "  - action, $A_{t}$\n",
        "  - reward, $R_{t+1}$\n",
        "  - value: $V_{\\phi}(S_{t})$\n",
        "  - log policy distribution (old): $\\log(\\pi_{\\theta}(A_{t}|S_{t}))$\n",
        "\n",
        "- Buffer 클래스에서 계산해줘야 하는 것들\n",
        "  - advantage:\n",
        "\n",
        "    $\\delta_{t}=R_{t+1}+\\gamma V_{\\phi}(S_{t+1}) - V_{\\phi}(S_{t})$\n",
        "\n",
        "    $A_{t}^{(\\lambda)}=\\delta_{t}+\\gamma \\lambda A_{t+1}^{(\\lambda)}$\n",
        "  - return:\n",
        "\n",
        "    $G_{t} = R_{t+1} + \\gamma G_{t+1}$\n",
        "\n",
        "- PPO는 On-policy 알고리즘으로 Policy의 업데이트 이후 Buffer 초기화 작업 필요\n",
        "  - Buffer의 크기 = Transition 수집 횟수\n",
        "  - 정해진 횟수 만큼 Transition을 모두 수집하면 이전에 수집된 데이터가 모두 업데이트 됨. 별도의 Refresh 작업이 필요하지 않음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHYaLUFxkh7s"
      },
      "outputs": [],
      "source": [
        "class PPOBuffer:\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
        "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
        "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.gamma, self.lam = gamma, lam\n",
        "        self.idx, self.path_start_idx, self.max_size = 0, 0, size\n",
        "\n",
        "    def store(self, obs, act, rew, val, logp):\n",
        "\n",
        "        self.obs_buf[self.idx] = obs\n",
        "        self.act_buf[self.idx] = act\n",
        "        self.rew_buf[self.idx] = rew\n",
        "        self.val_buf[self.idx] = val\n",
        "        self.logp_buf[self.idx] = logp\n",
        "        self.idx += 1\n",
        "\n",
        "    def finish_path(self, last_val=0):\n",
        "\n",
        "        path_slice = slice(self.path_start_idx, self.idx)\n",
        "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
        "        vals = np.append(self.val_buf[path_slice], last_val)\n",
        "\n",
        "        # the next two lines implement GAE-Lambda advantage calculation\n",
        "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
        "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
        "\n",
        "        # the next line computes rewards-to-go, to be targets for the value function\n",
        "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
        "\n",
        "        self.path_start_idx = self.idx\n",
        "\n",
        "    def get(self):\n",
        "        self.idx, self.path_start_idx = 0, 0\n",
        "        # the next two lines implement the advantage normalization trick\n",
        "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
        "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
        "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
        "                    adv=self.adv_buf, logp=self.logp_buf)\n",
        "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Network and Value Network\n",
        "- Policy network: Observation이 입력, $\\mu_{t}$를 출력으로 하는 네트워크 설계\n",
        "  - $\\mu_{t}=\\pi_{\\theta}(s)$\n",
        "  - $\\sigma_{t} = \\exp($log_std$_{t})$\n",
        "- Value network: Observation이 입력, 1차원 Value를 출력하는 네트워크 설계"
      ],
      "metadata": {
        "id": "eu4mXYiokd2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Util 함수들\n",
        "def combined_shape(length, shape=None):\n",
        "    if shape is None:\n",
        "        return (length,)\n",
        "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "def count_vars(module):\n",
        "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
      ],
      "metadata": {
        "id": "4KxCNhIZviFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch 네트워크 클래스 정의"
      ],
      "metadata": {
        "id": "FCYUvcbTtbRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(sizes, activation, output_activation=nn.Identity):\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class MLPGaussianActor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
        "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
        "        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
        "\n",
        "    def _distribution(self, obs):\n",
        "        mu = self.mu_net(obs)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return Normal(mu, std)\n",
        "\n",
        "    def _log_prob_from_distribution(self, pi, act):\n",
        "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
        "\n",
        "    def _get_mode(self,obs):\n",
        "        return self.mu_net(obs)\n",
        "\n",
        "    def forward(self, obs, act=None):\n",
        "        pi = self._distribution(obs)\n",
        "        logp_a = None\n",
        "        if act is not None:\n",
        "            logp_a = self._log_prob_from_distribution(pi, act)\n",
        "        return pi, logp_a\n",
        "\n",
        "\n",
        "class MLPCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
        "\n",
        "class MLPActorCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim,\n",
        "                 hidden_sizes=(64,64), activation=nn.Tanh):\n",
        "        super().__init__()\n",
        "\n",
        "        # policy builder depends on action space\n",
        "        self.pi = MLPGaussianActor(obs_dim, act_dim, hidden_sizes, activation)\n",
        "\n",
        "        # build value function\n",
        "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation)\n",
        "\n",
        "    def step(self, obs):\n",
        "        with torch.no_grad():\n",
        "            pi = self.pi._distribution(obs)\n",
        "            a = pi.sample()\n",
        "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
        "            v = self.v(obs)\n",
        "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
        "\n",
        "    def act(self, obs):\n",
        "        return self.pi._get_mode(obs).numpy()"
      ],
      "metadata": {
        "id": "Bqa5ajMFkein"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment 생성, Buffer 생성."
      ],
      "metadata": {
        "id": "15K76XnCtyZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = 4000\n",
        "gamma = 0.99\n",
        "lam = 0.97\n",
        "\n",
        "env = HalfCheetahEnv()\n",
        "obs_dim = env.observation_dim\n",
        "act_dim = env.action_dim\n",
        "\n",
        "# Set up experience buffer\n",
        "buf = PPOBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)"
      ],
      "metadata": {
        "id": "eISxiHEqtr9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actor Critic 네트워크 생성 및 Optimizer 초기화."
      ],
      "metadata": {
        "id": "yQ1KP0VIt7FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_sizes=[64,64]\n",
        "pi_lr = 3e-4\n",
        "vf_lr = 1e-3\n",
        "\n",
        "# Create actor-critic module\n",
        "ac = MLPActorCritic(env.observation_dim, env.action_dim, hidden_sizes)\n",
        "\n",
        "# Count variables\n",
        "var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
        "print('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
        "\n",
        "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
        "vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72d-lkUBt639",
        "outputId": "eb932b79-6c29-4d06-821b-0f5d91e14db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of parameters: \t pi: 5708, \t v: 5377\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "에피소드 수집.\n",
        "  - Environment 초기화. 초기 상태 $s_0$\n",
        "  - Policy 로부터 다음 정보 획득: $a_{t}, v(s_{t}), \\log(\\pi(a_{t}|s_{t}))$\n",
        "    - $a_{t} = \\mu_{t} + \\sigma_{t}\\epsilon_{t}$\n",
        "    - $v(s_{t})$: Advantage를 계산하기 위해 필요\n",
        "    - $\\log(\\pi(a_{t}|s_{t}))$: Ratio를 계산하기 위해 필요\n",
        "  - $s_{t+1}, r_{t+1}, d_{t}$ 획득\n",
        "  - Buffer에 저장\n",
        "  - 에피소드 종료 체크, 환경 초기화, $s_0$ 획득"
      ],
      "metadata": {
        "id": "0XJ_ZjRA1EMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_ep_len = 1000\n",
        "\n",
        "# Prepare for interaction with environment\n",
        "start_time = time.time()\n",
        "o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "# Main loop: collect experience in env and update/log each epoch\n",
        "for t in range(steps_per_epoch):\n",
        "    a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
        "\n",
        "    next_o, r, d, _ = env.step(a)\n",
        "    ep_ret += r\n",
        "    ep_len += 1\n",
        "\n",
        "    # save and log\n",
        "    buf.store(o, a, r, v, logp)\n",
        "\n",
        "    # Update obs (critical!)\n",
        "    o = next_o\n",
        "\n",
        "    timeout = ep_len == max_ep_len\n",
        "    terminal = d or timeout\n",
        "    epoch_ended = t==steps_per_epoch-1\n",
        "\n",
        "    if terminal or epoch_ended:\n",
        "      if timeout or epoch_ended:\n",
        "          _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
        "      else:\n",
        "          v = 0\n",
        "      buf.finish_path(v)\n",
        "      o, ep_ret, ep_len = env.reset(), 0, 0"
      ],
      "metadata": {
        "id": "sGXgXXDhvoS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buffer에서 data를 불러오기"
      ],
      "metadata": {
        "id": "vk3gNo2J0iah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = buf.get()\n",
        "obs, act, adv, logp_old, ret = data['obs'], data['act'], data['adv'], data['logp'], data['ret']"
      ],
      "metadata": {
        "id": "uxDWi2IzvoJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy 네트워크 업데이트. 이때 업데이트 된 kl divergence가 특정 threshold 이상이면 업데이트 중지."
      ],
      "metadata": {
        "id": "K1p2Nxag0iMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_kl=0.02\n",
        "clip_ratio = 0.2\n",
        "train_pi_iters = 80\n",
        "\n",
        "# Train policy with multiple steps of gradient descent\n",
        "for i in range(train_pi_iters):\n",
        "\n",
        "    # Policy loss\n",
        "    pi, logp = ac.pi(obs, act)\n",
        "    ratio = torch.exp(logp - logp_old)\n",
        "    clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
        "    loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
        "\n",
        "    pi_optimizer.zero_grad()\n",
        "    approx_kl = (logp_old - logp).mean().item()\n",
        "    kl = np.mean(approx_kl)\n",
        "    if kl > 1.5 * target_kl:\n",
        "        print('Early stopping at step %d due to reaching max kl.'%i)\n",
        "        break\n",
        "    loss_pi.backward()\n",
        "    pi_optimizer.step()"
      ],
      "metadata": {
        "id": "l_nEou5Qvn7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value 네트워크 업데이트"
      ],
      "metadata": {
        "id": "RZg99VUd0hhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_v_iters=80\n",
        "\n",
        "# Value function learning\n",
        "for i in range(train_v_iters):\n",
        "    vf_optimizer.zero_grad()\n",
        "    loss_v = ((ac.v(obs) - ret)**2).mean()\n",
        "    loss_v.backward()\n",
        "    vf_optimizer.step()"
      ],
      "metadata": {
        "id": "YAlauNwSxnfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPO\n",
        "\n",
        "- 앞서 만든 클래스들과 함수들을 모두 합쳐 PPO 알고리즘 구현\n",
        "- Policy Loss: Clipped Surrogated Loss\n",
        "  - $L_{CLIP}(\\theta):=\\sum_{t=1}^{B} \\min( r_{t}(\\theta)A_{t},$clip$(r_{t}(\\theta),1-\\epsilon, 1+\\epsilon)A_{t})$\n",
        "  - $r_{t}(\\theta)=\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{k}}(a_{t}|s_{t})}$: $\\pi_{\\theta_{k}}(a_{t}|s_{t})$를 미리 Buffer에 저장해두기\n",
        "- Value Loss\n",
        "  - $L_{V}(\\phi):=\\sum_{t=1}^{B} (G_{t} - V_{\\phi}(s_{t}))^{2}$"
      ],
      "metadata": {
        "id": "rHYY-ARdkfP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvaQ1dVaQzjH"
      },
      "outputs": [],
      "source": [
        "def ppo(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0,\n",
        "        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
        "        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
        "        target_kl=0.01, save_freq=10):\n",
        "\n",
        "    # Random seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    EpRet = []\n",
        "    EpLen = []\n",
        "    VVals = []\n",
        "    TotalEnvInteracts = []\n",
        "    LossPi = []\n",
        "    LossV = []\n",
        "    DeltaLossPi = []\n",
        "    DeltaLossV = []\n",
        "    Entropy = []\n",
        "    KL = []\n",
        "    ClipFrac = []\n",
        "    StopIter = []\n",
        "    Time = []\n",
        "\n",
        "    # Instantiate environment\n",
        "    env = env_fn()\n",
        "    obs_dim = env.observation_dim\n",
        "    act_dim = env.action_dim\n",
        "\n",
        "    # Create actor-critic module\n",
        "    ac = actor_critic(env.observation_dim, env.action_dim, **ac_kwargs)\n",
        "\n",
        "    # Count variables\n",
        "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
        "    print('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
        "\n",
        "    # Set up experience buffer\n",
        "    buf = PPOBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
        "\n",
        "    # Set up function for computing PPO policy loss\n",
        "    def compute_loss_pi(data):\n",
        "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
        "\n",
        "        # Policy loss\n",
        "        pi, logp = ac.pi(obs, act)\n",
        "        ratio = torch.exp(logp - logp_old)\n",
        "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
        "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
        "\n",
        "        # Useful extra info\n",
        "        approx_kl = (logp_old - logp).mean().item()\n",
        "        ent = pi.entropy().mean().item()\n",
        "        clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
        "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
        "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
        "\n",
        "        return loss_pi, pi_info\n",
        "\n",
        "    # Set up function for computing value loss\n",
        "    def compute_loss_v(data):\n",
        "        obs, ret = data['obs'], data['ret']\n",
        "        return ((ac.v(obs) - ret)**2).mean()\n",
        "\n",
        "    # Set up optimizers for policy and value function\n",
        "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
        "    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
        "\n",
        "    def update():\n",
        "        data = buf.get()\n",
        "\n",
        "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
        "        pi_l_old = pi_l_old.item()\n",
        "        v_l_old = compute_loss_v(data).item()\n",
        "\n",
        "        # Train policy with multiple steps of gradient descent\n",
        "        for i in range(train_pi_iters):\n",
        "            pi_optimizer.zero_grad()\n",
        "            loss_pi, pi_info = compute_loss_pi(data)\n",
        "            kl = np.mean(pi_info['kl'])\n",
        "            if kl > 1.5 * target_kl:\n",
        "                print('Early stopping at step %d due to reaching max kl.'%i)\n",
        "                break\n",
        "            loss_pi.backward()\n",
        "            pi_optimizer.step()\n",
        "\n",
        "        # Value function learning\n",
        "        for i in range(train_v_iters):\n",
        "            vf_optimizer.zero_grad()\n",
        "            loss_v = compute_loss_v(data)\n",
        "            loss_v.backward()\n",
        "            vf_optimizer.step()\n",
        "\n",
        "        # Log changes from update\n",
        "        LossPi.append(pi_l_old)\n",
        "        LossV.append(v_l_old)\n",
        "        KL.append(pi_info['kl'])\n",
        "        Entropy.append(pi_info_old['ent'])\n",
        "        ClipFrac.append(pi_info['cf'])\n",
        "        DeltaLossPi.append(loss_pi.item() - pi_l_old)\n",
        "        DeltaLossV.append(loss_v.item() - v_l_old)\n",
        "\n",
        "    # Prepare for interaction with environment\n",
        "    start_time = time.time()\n",
        "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "    # Main loop: collect experience in env and update/log each epoch\n",
        "    for epoch in range(epochs):\n",
        "        for t in range(steps_per_epoch):\n",
        "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
        "\n",
        "            next_o, r, d, _ = env.step(a)\n",
        "            ep_ret += r\n",
        "            ep_len += 1\n",
        "\n",
        "            # save and log\n",
        "            buf.store(o, a, r, v, logp)\n",
        "            VVals.append(v)\n",
        "\n",
        "            # Update obs (critical!)\n",
        "            o = next_o\n",
        "\n",
        "            timeout = ep_len == max_ep_len\n",
        "            terminal = d or timeout\n",
        "            epoch_ended = t==steps_per_epoch-1\n",
        "\n",
        "            if terminal or epoch_ended:\n",
        "                if epoch_ended and not(terminal):\n",
        "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
        "                # if trajectory didn't reach terminal state, bootstrap value target\n",
        "                if timeout or epoch_ended:\n",
        "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
        "                else:\n",
        "                    v = 0\n",
        "                buf.finish_path(v)\n",
        "                if terminal:\n",
        "                    EpRet.append(ep_ret)\n",
        "                    EpLen.append(ep_len)\n",
        "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "        # Perform PPO update!\n",
        "        update()\n",
        "\n",
        "        TotalEnvInteracts.append((epoch+1)*steps_per_epoch)\n",
        "        Time.append(time.time()-start_time)\n",
        "\n",
        "        print(f'[Epoch:{epoch}] EpRet:{np.min(EpRet[-10:]):8.2f} < {np.mean(EpRet[-10:]):8.2f} < {np.max(EpRet[-10:]):8.2f}, EpLen:{np.mean(EpLen[-10:]):8.2f}, VVals:{np.mean(VVals[-10:]):8.2f}, TotalEnvInteracts:{TotalEnvInteracts[-1]:8d}, LossPi:{np.mean(LossPi[-10:]):8.2f}, LossV:{np.mean(LossV[-10:]):8.2f}, Entropy:{np.mean(Entropy[-10:]):8.2f}, KL:{np.mean(KL[-10:]):8.2f}, Time:{Time[-1]:8.2f}')\n",
        "    return ac, EpRet, EpLen, VVals, TotalEnvInteracts, LossPi, LossV, Entropy, KL, Time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run PPO"
      ],
      "metadata": {
        "id": "wBRomo8UzpYI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymyQwp7-SXHe",
        "outputId": "5cb7dfd7-6485-4257-9498-6158524de3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of parameters: \t pi: 5708, \t v: 5377\n",
            "\n",
            "[Epoch:0] EpRet: -438.42 <  -278.33 <  -158.13, EpLen: 1000.00, VVals:   -0.05, TotalEnvInteracts:    5000, LossPi:    0.00, LossV: 1014.67, Entropy:    0.92, KL:    0.01, Time:    5.13\n",
            "Early stopping at step 29 due to reaching max kl.\n",
            "[Epoch:1] EpRet: -438.42 <  -278.91 <  -158.13, EpLen: 1000.00, VVals:   -5.45, TotalEnvInteracts:   10000, LossPi:    0.00, LossV:  759.97, Entropy:    0.91, KL:    0.02, Time:    9.70\n",
            "Early stopping at step 55 due to reaching max kl.\n",
            "[Epoch:2] EpRet: -325.14 <  -259.48 <  -138.61, EpLen: 1000.00, VVals:  -16.03, TotalEnvInteracts:   15000, LossPi:    0.00, LossV:  575.38, Entropy:    0.91, KL:    0.02, Time:   15.32\n",
            "[Epoch:3] EpRet: -284.86 <  -234.16 <  -138.61, EpLen: 1000.00, VVals:  -18.81, TotalEnvInteracts:   20000, LossPi:    0.00, LossV:  479.04, Entropy:    0.91, KL:    0.01, Time:   20.37\n",
            "Early stopping at step 6 due to reaching max kl.\n",
            "[Epoch:4] EpRet: -358.64 <  -218.28 <  -106.98, EpLen: 1000.00, VVals:  -21.33, TotalEnvInteracts:   25000, LossPi:    0.00, LossV:  410.66, Entropy:    0.91, KL:    0.02, Time:   25.71\n",
            "Early stopping at step 6 due to reaching max kl.\n",
            "[Epoch:5] EpRet: -358.64 <  -245.89 <  -106.98, EpLen: 1000.00, VVals:  -21.96, TotalEnvInteracts:   30000, LossPi:    0.00, LossV:  369.60, Entropy:    0.90, KL:    0.02, Time:   30.02\n",
            "Early stopping at step 44 due to reaching max kl.\n",
            "[Epoch:6] EpRet: -355.31 <  -260.37 <  -178.33, EpLen: 1000.00, VVals:  -25.34, TotalEnvInteracts:   35000, LossPi:    0.00, LossV:  330.25, Entropy:    0.90, KL:    0.02, Time:   34.73\n",
            "Early stopping at step 5 due to reaching max kl.\n",
            "[Epoch:7] EpRet: -305.47 <  -223.50 <  -101.21, EpLen: 1000.00, VVals:  -23.94, TotalEnvInteracts:   40000, LossPi:    0.00, LossV:  308.90, Entropy:    0.90, KL:    0.02, Time:   39.92\n",
            "[Epoch:8] EpRet: -305.47 <  -188.53 <  -101.21, EpLen: 1000.00, VVals:  -23.65, TotalEnvInteracts:   45000, LossPi:    0.00, LossV:  287.18, Entropy:    0.90, KL:    0.02, Time:   45.00\n",
            "Early stopping at step 39 due to reaching max kl.\n",
            "[Epoch:9] EpRet: -251.46 <  -157.91 <   -63.70, EpLen: 1000.00, VVals:  -23.73, TotalEnvInteracts:   50000, LossPi:    0.00, LossV:  269.92, Entropy:    0.90, KL:    0.02, Time:   50.23\n",
            "Early stopping at step 41 due to reaching max kl.\n",
            "[Epoch:10] EpRet: -257.42 <  -170.29 <   -63.70, EpLen: 1000.00, VVals:  -14.95, TotalEnvInteracts:   55000, LossPi:    0.00, LossV:  182.78, Entropy:    0.89, KL:    0.02, Time:   55.32\n",
            "Early stopping at step 36 due to reaching max kl.\n",
            "[Epoch:11] EpRet: -257.42 <  -187.62 <   -67.51, EpLen: 1000.00, VVals:  -24.20, TotalEnvInteracts:   60000, LossPi:    0.00, LossV:  146.88, Entropy:    0.89, KL:    0.02, Time:   59.94\n",
            "Early stopping at step 34 due to reaching max kl.\n",
            "[Epoch:12] EpRet: -236.20 <  -167.57 <   -67.51, EpLen: 1000.00, VVals:  -23.87, TotalEnvInteracts:   65000, LossPi:    0.00, LossV:  135.01, Entropy:    0.88, KL:    0.02, Time:   65.37\n",
            "Early stopping at step 7 due to reaching max kl.\n",
            "[Epoch:13] EpRet: -208.00 <  -152.32 <   -68.54, EpLen: 1000.00, VVals:  -12.57, TotalEnvInteracts:   70000, LossPi:   -0.00, LossV:  126.87, Entropy:    0.88, KL:    0.02, Time:   69.70\n",
            "Early stopping at step 35 due to reaching max kl.\n",
            "[Epoch:14] EpRet: -210.21 <  -151.59 <   -68.54, EpLen: 1000.00, VVals:  -11.82, TotalEnvInteracts:   75000, LossPi:   -0.00, LossV:  120.50, Entropy:    0.88, KL:    0.02, Time:   74.34\n",
            "Early stopping at step 60 due to reaching max kl.\n",
            "[Epoch:15] EpRet: -210.21 <  -136.24 <   -45.35, EpLen: 1000.00, VVals:  -23.90, TotalEnvInteracts:   80000, LossPi:   -0.00, LossV:  113.17, Entropy:    0.87, KL:    0.02, Time:   80.02\n",
            "Early stopping at step 30 due to reaching max kl.\n",
            "[Epoch:16] EpRet: -200.82 <  -112.99 <   -45.35, EpLen: 1000.00, VVals:  -23.45, TotalEnvInteracts:   85000, LossPi:   -0.00, LossV:  110.84, Entropy:    0.87, KL:    0.02, Time:   84.59\n",
            "Early stopping at step 63 due to reaching max kl.\n",
            "[Epoch:17] EpRet: -158.41 <  -100.00 <   -42.74, EpLen: 1000.00, VVals:  -16.19, TotalEnvInteracts:   90000, LossPi:   -0.00, LossV:  105.76, Entropy:    0.86, KL:    0.02, Time:   90.49\n",
            "Early stopping at step 6 due to reaching max kl.\n",
            "[Epoch:18] EpRet: -189.69 <  -128.04 <   -42.74, EpLen: 1000.00, VVals:   -2.57, TotalEnvInteracts:   95000, LossPi:   -0.00, LossV:  106.53, Entropy:    0.86, KL:    0.02, Time:   94.80\n",
            "Early stopping at step 38 due to reaching max kl.\n",
            "[Epoch:19] EpRet: -206.61 <  -142.42 <   -61.66, EpLen: 1000.00, VVals:  -24.39, TotalEnvInteracts:  100000, LossPi:   -0.00, LossV:  103.11, Entropy:    0.85, KL:    0.02, Time:   99.46\n",
            "Early stopping at step 31 due to reaching max kl.\n",
            "[Epoch:20] EpRet: -206.61 <  -131.07 <   -61.66, EpLen: 1000.00, VVals:  -21.95, TotalEnvInteracts:  105000, LossPi:   -0.00, LossV:   97.92, Entropy:    0.85, KL:    0.02, Time:  104.89\n",
            "Early stopping at step 41 due to reaching max kl.\n",
            "[Epoch:21] EpRet: -209.46 <  -146.07 <  -105.34, EpLen: 1000.00, VVals:   -8.42, TotalEnvInteracts:  110000, LossPi:   -0.00, LossV:   91.60, Entropy:    0.84, KL:    0.02, Time:  109.58\n",
            "Early stopping at step 6 due to reaching max kl.\n",
            "[Epoch:22] EpRet: -209.46 <  -131.84 <   -75.68, EpLen: 1000.00, VVals:  -23.10, TotalEnvInteracts:  115000, LossPi:   -0.00, LossV:   91.32, Entropy:    0.84, KL:    0.02, Time:  114.51\n",
            "Early stopping at step 39 due to reaching max kl.\n",
            "[Epoch:23] EpRet: -186.41 <  -100.14 <   -16.50, EpLen: 1000.00, VVals:  -14.68, TotalEnvInteracts:  120000, LossPi:   -0.00, LossV:   93.63, Entropy:    0.84, KL:    0.02, Time:  119.59\n",
            "Early stopping at step 32 due to reaching max kl.\n",
            "[Epoch:24] EpRet: -193.04 <  -105.02 <   -16.50, EpLen: 1000.00, VVals:  -21.15, TotalEnvInteracts:  125000, LossPi:   -0.00, LossV:   94.43, Entropy:    0.83, KL:    0.02, Time:  124.17\n",
            "Early stopping at step 33 due to reaching max kl.\n",
            "[Epoch:25] EpRet: -193.04 <   -94.12 <   -20.77, EpLen: 1000.00, VVals:   -9.73, TotalEnvInteracts:  130000, LossPi:   -0.00, LossV:   92.90, Entropy:    0.83, KL:    0.02, Time:  129.62\n",
            "Early stopping at step 5 due to reaching max kl.\n",
            "[Epoch:26] EpRet: -153.95 <   -73.86 <   -20.77, EpLen: 1000.00, VVals:  -11.50, TotalEnvInteracts:  135000, LossPi:   -0.00, LossV:   94.67, Entropy:    0.82, KL:    0.02, Time:  133.95\n",
            "Early stopping at step 23 due to reaching max kl.\n",
            "[Epoch:27] EpRet: -198.13 <   -84.58 <    51.47, EpLen: 1000.00, VVals:  -18.49, TotalEnvInteracts:  140000, LossPi:   -0.00, LossV:   95.45, Entropy:    0.82, KL:    0.02, Time:  139.42\n",
            "Early stopping at step 21 due to reaching max kl.\n",
            "[Epoch:28] EpRet: -198.13 <   -58.79 <    56.64, EpLen: 1000.00, VVals:   -6.97, TotalEnvInteracts:  145000, LossPi:   -0.00, LossV:   92.07, Entropy:    0.81, KL:    0.02, Time:  144.81\n",
            "Early stopping at step 30 due to reaching max kl.\n",
            "[Epoch:29] EpRet: -199.02 <   -51.72 <    56.64, EpLen: 1000.00, VVals:   -3.30, TotalEnvInteracts:  150000, LossPi:   -0.00, LossV:   89.29, Entropy:    0.81, KL:    0.02, Time:  149.35\n",
            "Early stopping at step 37 due to reaching max kl.\n",
            "[Epoch:30] EpRet: -199.02 <   -50.53 <    30.93, EpLen: 1000.00, VVals:   -2.53, TotalEnvInteracts:  155000, LossPi:   -0.00, LossV:   91.83, Entropy:    0.80, KL:    0.02, Time:  154.89\n",
            "[Epoch:31] EpRet: -129.97 <   -12.06 <    54.53, EpLen: 1000.00, VVals:   -4.80, TotalEnvInteracts:  160000, LossPi:   -0.00, LossV:   97.45, Entropy:    0.80, KL:    0.02, Time:  160.10\n",
            "Early stopping at step 31 due to reaching max kl.\n",
            "[Epoch:32] EpRet:  -85.70 <    22.26 <   103.43, EpLen: 1000.00, VVals:   -4.65, TotalEnvInteracts:  165000, LossPi:   -0.00, LossV:  100.40, Entropy:    0.79, KL:    0.02, Time:  164.68\n",
            "Early stopping at step 32 due to reaching max kl.\n",
            "[Epoch:33] EpRet:  -81.73 <    18.87 <   103.43, EpLen: 1000.00, VVals:    1.51, TotalEnvInteracts:  170000, LossPi:   -0.00, LossV:  101.36, Entropy:    0.79, KL:    0.02, Time:  170.12\n",
            "Early stopping at step 5 due to reaching max kl.\n",
            "[Epoch:34] EpRet:  -81.73 <    15.65 <    67.57, EpLen: 1000.00, VVals:   -2.30, TotalEnvInteracts:  175000, LossPi:   -0.00, LossV:  102.57, Entropy:    0.78, KL:    0.02, Time:  174.44\n",
            "Early stopping at step 6 due to reaching max kl.\n",
            "[Epoch:35] EpRet:   10.65 <    65.94 <   136.95, EpLen: 1000.00, VVals:   -0.55, TotalEnvInteracts:  180000, LossPi:    0.00, LossV:  104.50, Entropy:    0.78, KL:    0.02, Time:  179.11\n",
            "Early stopping at step 6 due to reaching max kl.\n",
            "[Epoch:36] EpRet:    1.14 <    95.23 <   139.78, EpLen: 1000.00, VVals:    6.94, TotalEnvInteracts:  185000, LossPi:    0.00, LossV:  107.31, Entropy:    0.77, KL:    0.02, Time:  183.97\n",
            "Early stopping at step 61 due to reaching max kl.\n",
            "[Epoch:37] EpRet:    1.14 <    79.47 <   139.78, EpLen: 1000.00, VVals:   11.94, TotalEnvInteracts:  190000, LossPi:    0.00, LossV:  114.35, Entropy:    0.77, KL:    0.02, Time:  188.87\n",
            "Early stopping at step 43 due to reaching max kl.\n",
            "[Epoch:38] EpRet:   24.16 <    82.27 <   189.93, EpLen: 1000.00, VVals:   13.78, TotalEnvInteracts:  195000, LossPi:    0.00, LossV:  119.14, Entropy:    0.76, KL:    0.02, Time:  194.44\n",
            "Early stopping at step 51 due to reaching max kl.\n",
            "[Epoch:39] EpRet: -119.09 <    54.69 <   189.93, EpLen: 1000.00, VVals:   23.51, TotalEnvInteracts:  200000, LossPi:    0.00, LossV:  130.40, Entropy:    0.75, KL:    0.02, Time:  199.25\n",
            "Early stopping at step 32 due to reaching max kl.\n",
            "[Epoch:40] EpRet: -119.09 <    68.16 <   183.22, EpLen: 1000.00, VVals:    8.37, TotalEnvInteracts:  205000, LossPi:    0.00, LossV:  131.66, Entropy:    0.75, KL:    0.02, Time:  203.84\n",
            "Early stopping at step 67 due to reaching max kl.\n",
            "[Epoch:41] EpRet:   49.90 <   128.45 <   214.94, EpLen: 1000.00, VVals:   14.91, TotalEnvInteracts:  210000, LossPi:    0.00, LossV:  130.36, Entropy:    0.74, KL:    0.02, Time:  209.61\n",
            "Early stopping at step 28 due to reaching max kl.\n",
            "[Epoch:42] EpRet:   49.90 <   144.91 <   219.69, EpLen: 1000.00, VVals:   16.05, TotalEnvInteracts:  215000, LossPi:    0.00, LossV:  134.74, Entropy:    0.73, KL:    0.02, Time:  214.34\n",
            "Early stopping at step 34 due to reaching max kl.\n",
            "[Epoch:43] EpRet:  107.87 <   185.84 <   254.92, EpLen: 1000.00, VVals:   16.64, TotalEnvInteracts:  220000, LossPi:    0.00, LossV:  136.51, Entropy:    0.73, KL:    0.02, Time:  219.95\n",
            "Early stopping at step 4 due to reaching max kl.\n",
            "[Epoch:44] EpRet:   22.81 <   194.91 <   302.18, EpLen: 1000.00, VVals:   15.61, TotalEnvInteracts:  225000, LossPi:    0.00, LossV:  145.17, Entropy:    0.72, KL:    0.02, Time:  224.24\n",
            "Early stopping at step 5 due to reaching max kl.\n",
            "[Epoch:45] EpRet:  -17.73 <   186.12 <   302.18, EpLen: 1000.00, VVals:   -7.33, TotalEnvInteracts:  230000, LossPi:   -0.00, LossV:  157.66, Entropy:    0.71, KL:    0.02, Time:  228.54\n",
            "Early stopping at step 43 due to reaching max kl.\n",
            "[Epoch:46] EpRet:  -17.73 <   193.80 <   276.27, EpLen: 1000.00, VVals:   17.05, TotalEnvInteracts:  235000, LossPi:   -0.00, LossV:  159.79, Entropy:    0.71, KL:    0.02, Time:  234.06\n",
            "Early stopping at step 26 due to reaching max kl.\n",
            "[Epoch:47] EpRet:  129.58 <   198.21 <   266.65, EpLen: 1000.00, VVals:   23.37, TotalEnvInteracts:  240000, LossPi:    0.00, LossV:  158.18, Entropy:    0.70, KL:    0.02, Time:  238.54\n",
            "[Epoch:48] EpRet:  129.58 <   216.33 <   333.05, EpLen: 1000.00, VVals:   24.47, TotalEnvInteracts:  245000, LossPi:   -0.00, LossV:  160.27, Entropy:    0.69, KL:    0.02, Time:  244.09\n",
            "Early stopping at step 24 due to reaching max kl.\n",
            "[Epoch:49] EpRet:  136.05 <   247.44 <   377.01, EpLen: 1000.00, VVals:  -22.26, TotalEnvInteracts:  250000, LossPi:   -0.00, LossV:  162.76, Entropy:    0.68, KL:    0.02, Time:  248.99\n"
          ]
        }
      ],
      "source": [
        "ac = ppo(lambda : HalfCheetahEnv(), actor_critic=MLPActorCritic,\n",
        "         ac_kwargs=dict(hidden_sizes=[64,64]), gamma=0.99,\n",
        "         seed=0, steps_per_epoch=5000, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "LaCAjhYkofaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = HalfCheetahEnv()\n",
        "imgs = []\n",
        "\n",
        "obs = env.reset()\n",
        "for t in range(1000):\n",
        "  with torch.no_grad():\n",
        "    obs = torch.as_tensor(obs, dtype=torch.float32)\n",
        "    action = ac[0].act(obs)\n",
        "  obs, reward, terminated, info = env.step(action)\n",
        "  img = env.render()\n",
        "  imgs.append(img)\n",
        "\n",
        "media.show_video(imgs, fps=1/env.dt)"
      ],
      "metadata": {
        "id": "65tLhW-cZdBv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "78597b65-9fe5-4f4d-a204-d71bd1d3cc49"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}