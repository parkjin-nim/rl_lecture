{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDfQ6Y1N0XF1"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eRZ0IFzjYLh",
        "outputId": "39b4b932-212c-4f30-ac25-062fb03fab1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25henv: MUJOCO_GL=egl\n"
          ]
        }
      ],
      "source": [
        "!pip install -q numpy\n",
        "!pip install -q matplotlib\n",
        "!pip install -q mujoco\n",
        "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "!pip install -q mediapy\n",
        "\n",
        "%env MUJOCO_GL=egl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnusYbcsjxQ4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import mediapy as media\n",
        "import matplotlib.pyplot as plt\n",
        "import mujoco\n",
        "from copy import deepcopy\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import scipy.signal\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions.normal import Normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB1Joyof0aiM"
      },
      "source": [
        "### Define MuJoCo Enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptBa-X7Djx2Y"
      },
      "outputs": [],
      "source": [
        "xml_string=\"\"\"<!-- Cheetah Model\n",
        "\n",
        "    The state space is populated with joints in the order that they are\n",
        "    defined in this file. The actuators also operate on joints.\n",
        "\n",
        "    State-Space (name/joint/parameter):\n",
        "        - rootx     slider      position (m)\n",
        "        - rootz     slider      position (m)\n",
        "        - rooty     hinge       angle (rad)\n",
        "        - bthigh    hinge       angle (rad)\n",
        "        - bshin     hinge       angle (rad)\n",
        "        - bfoot     hinge       angle (rad)\n",
        "        - fthigh    hinge       angle (rad)\n",
        "        - fshin     hinge       angle (rad)\n",
        "        - ffoot     hinge       angle (rad)\n",
        "        - rootx     slider      velocity (m/s)\n",
        "        - rootz     slider      velocity (m/s)\n",
        "        - rooty     hinge       angular velocity (rad/s)\n",
        "        - bthigh    hinge       angular velocity (rad/s)\n",
        "        - bshin     hinge       angular velocity (rad/s)\n",
        "        - bfoot     hinge       angular velocity (rad/s)\n",
        "        - fthigh    hinge       angular velocity (rad/s)\n",
        "        - fshin     hinge       angular velocity (rad/s)\n",
        "        - ffoot     hinge       angular velocity (rad/s)\n",
        "\n",
        "    Actuators (name/actuator/parameter):\n",
        "        - bthigh    hinge       torque (N m)\n",
        "        - bshin     hinge       torque (N m)\n",
        "        - bfoot     hinge       torque (N m)\n",
        "        - fthigh    hinge       torque (N m)\n",
        "        - fshin     hinge       torque (N m)\n",
        "        - ffoot     hinge       torque (N m)\n",
        "\n",
        "-->\n",
        "<mujoco model=\"cheetah\">\n",
        "  <compiler angle=\"radian\" coordinate=\"local\" inertiafromgeom=\"true\" settotalmass=\"14\"/>\n",
        "  <default>\n",
        "    <joint armature=\".1\" damping=\".01\" limited=\"true\" solimplimit=\"0 .8 .03\" solreflimit=\".02 1\" stiffness=\"8\"/>\n",
        "    <geom conaffinity=\"0\" condim=\"3\" contype=\"1\" friction=\".4 .1 .1\" rgba=\"0.8 0.6 .4 1\" solimp=\"0.0 0.8 0.01\" solref=\"0.02 1\"/>\n",
        "    <motor ctrllimited=\"true\" ctrlrange=\"-1 1\"/>\n",
        "  </default>\n",
        "  <size nstack=\"300000\" nuser_geom=\"1\"/>\n",
        "  <option gravity=\"0 0 -9.81\" timestep=\"0.01\"/>\n",
        "  <asset>\n",
        "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"1 1 1\" rgb2=\"0 0 0\" type=\"skybox\" width=\"100\"/>\n",
        "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
        "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0 0 0\" rgb2=\"0.8 0.8 0.8\" type=\"2d\" width=\"100\"/>\n",
        "    <material name=\"MatPlane\" reflectance=\"0.5\" shininess=\"1\" specular=\"1\" texrepeat=\"60 60\" texture=\"texplane\"/>\n",
        "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
        "  </asset>\n",
        "  <worldbody>\n",
        "    <light cutoff=\"100\" diffuse=\"1 1 1\" dir=\"-0 0 -1.3\" directional=\"true\" exponent=\"1\" pos=\"0 0 1.3\" specular=\".1 .1 .1\"/>\n",
        "    <geom conaffinity=\"1\" condim=\"3\" material=\"MatPlane\" name=\"floor\" pos=\"0 0 0\" rgba=\"0.8 0.9 0.8 1\" size=\"40 40 40\" type=\"plane\"/>\n",
        "    <body name=\"torso\" pos=\"0 0 .7\">\n",
        "      <camera name=\"track\" mode=\"trackcom\" pos=\"0 -3 0.3\" xyaxes=\"1 0 0 0 0 1\"/>\n",
        "      <joint armature=\"0\" axis=\"1 0 0\" damping=\"0\" limited=\"false\" name=\"rootx\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
        "      <joint armature=\"0\" axis=\"0 0 1\" damping=\"0\" limited=\"false\" name=\"rootz\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
        "      <joint armature=\"0\" axis=\"0 1 0\" damping=\"0\" limited=\"false\" name=\"rooty\" pos=\"0 0 0\" stiffness=\"0\" type=\"hinge\"/>\n",
        "      <geom fromto=\"-.5 0 0 .5 0 0\" name=\"torso\" size=\"0.046\" type=\"capsule\"/>\n",
        "      <geom axisangle=\"0 1 0 .87\" name=\"head\" pos=\".6 0 .1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
        "      <!-- <site name='tip'  pos='.15 0 .11'/>-->\n",
        "      <body name=\"bthigh\" pos=\"-.5 0 0\">\n",
        "        <joint axis=\"0 1 0\" damping=\"6\" name=\"bthigh\" pos=\"0 0 0\" range=\"-.52 1.05\" stiffness=\"240\" type=\"hinge\"/>\n",
        "        <geom axisangle=\"0 1 0 -3.8\" name=\"bthigh\" pos=\".1 0 -.13\" size=\"0.046 .145\" type=\"capsule\"/>\n",
        "        <body name=\"bshin\" pos=\".16 0 -.25\">\n",
        "          <joint axis=\"0 1 0\" damping=\"4.5\" name=\"bshin\" pos=\"0 0 0\" range=\"-.785 .785\" stiffness=\"180\" type=\"hinge\"/>\n",
        "          <geom axisangle=\"0 1 0 -2.03\" name=\"bshin\" pos=\"-.14 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
        "          <body name=\"bfoot\" pos=\"-.28 0 -.14\">\n",
        "            <joint axis=\"0 1 0\" damping=\"3\" name=\"bfoot\" pos=\"0 0 0\" range=\"-.4 .785\" stiffness=\"120\" type=\"hinge\"/>\n",
        "            <geom axisangle=\"0 1 0 -.27\" name=\"bfoot\" pos=\".03 0 -.097\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .094\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "      <body name=\"fthigh\" pos=\".5 0 0\">\n",
        "        <joint axis=\"0 1 0\" damping=\"4.5\" name=\"fthigh\" pos=\"0 0 0\" range=\"-1 .7\" stiffness=\"180\" type=\"hinge\"/>\n",
        "        <geom axisangle=\"0 1 0 .52\" name=\"fthigh\" pos=\"-.07 0 -.12\" size=\"0.046 .133\" type=\"capsule\"/>\n",
        "        <body name=\"fshin\" pos=\"-.14 0 -.24\">\n",
        "          <joint axis=\"0 1 0\" damping=\"3\" name=\"fshin\" pos=\"0 0 0\" range=\"-1.2 .87\" stiffness=\"120\" type=\"hinge\"/>\n",
        "          <geom axisangle=\"0 1 0 -.6\" name=\"fshin\" pos=\".065 0 -.09\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .106\" type=\"capsule\"/>\n",
        "          <body name=\"ffoot\" pos=\".13 0 -.18\">\n",
        "            <joint axis=\"0 1 0\" damping=\"1.5\" name=\"ffoot\" pos=\"0 0 0\" range=\"-.5 .5\" stiffness=\"60\" type=\"hinge\"/>\n",
        "            <geom axisangle=\"0 1 0 -.6\" name=\"ffoot\" pos=\".045 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .07\" type=\"capsule\"/>\n",
        "          </body>\n",
        "        </body>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <motor gear=\"120\" joint=\"bthigh\" name=\"bthigh\"/>\n",
        "    <motor gear=\"90\" joint=\"bshin\" name=\"bshin\"/>\n",
        "    <motor gear=\"60\" joint=\"bfoot\" name=\"bfoot\"/>\n",
        "    <motor gear=\"120\" joint=\"fthigh\" name=\"fthigh\"/>\n",
        "    <motor gear=\"60\" joint=\"fshin\" name=\"fshin\"/>\n",
        "    <motor gear=\"30\" joint=\"ffoot\" name=\"ffoot\"/>\n",
        "  </actuator>\n",
        "</mujoco>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRuOXImojyOi"
      },
      "outputs": [],
      "source": [
        "class HalfCheetahEnv():\n",
        "  def __init__(\n",
        "      self,\n",
        "      frame_skip=5,\n",
        "      forward_reward_weight=1.0,\n",
        "      ctrl_cost_weight=0.1,\n",
        "      reset_noise_scale=0.1\n",
        "      ):\n",
        "\n",
        "    self.frame_skip = frame_skip\n",
        "    self.forward_reward_weight = forward_reward_weight\n",
        "    self.ctrl_cost_weight = ctrl_cost_weight\n",
        "    self.reset_noise_scale = reset_noise_scale\n",
        "\n",
        "    self.initialize_simulation()\n",
        "    self.init_qpos = self.data.qpos.ravel().copy()\n",
        "    self.init_qvel = self.data.qvel.ravel().copy()\n",
        "    self.dt = self.model.opt.timestep * self.frame_skip\n",
        "\n",
        "    self.observation_dim = 17\n",
        "    self.action_dim = 6\n",
        "    self.action_limit = 1.\n",
        "\n",
        "  def initialize_simulation(self):\n",
        "    self.model = mujoco.MjModel.from_xml_string(xml_string)\n",
        "    self.data = mujoco.MjData(self.model)\n",
        "    mujoco.mj_resetData(self.model, self.data)\n",
        "    self.renderer = mujoco.Renderer(self.model)\n",
        "\n",
        "  def reset_simulation(self):\n",
        "    mujoco.mj_resetData(self.model, self.data)\n",
        "\n",
        "  def step_mujoco_simulation(self, ctrl, n_frames):\n",
        "    self.data.ctrl[:] = ctrl\n",
        "    mujoco.mj_step(self.model, self.data, nstep=n_frames)\n",
        "    self.renderer.update_scene(self.data,0)\n",
        "\n",
        "  def set_state(self, qpos, qvel):\n",
        "    self.data.qpos[:] = np.copy(qpos)\n",
        "    self.data.qvel[:] = np.copy(qvel)\n",
        "    if self.model.na == 0:\n",
        "      self.data.act[:] = None\n",
        "    mujoco.mj_forward(self.model, self.data)\n",
        "\n",
        "  def sample_action(self):\n",
        "    return (2.*np.random.uniform(size=(self.action_dim,)) - 1)*self.action_limit\n",
        "\n",
        "  def step(self, action):\n",
        "    x_position_before = self.data.qpos[0]\n",
        "    self.step_mujoco_simulation(action, self.frame_skip)\n",
        "    x_position_after = self.data.qpos[0]\n",
        "    x_velocity = (x_position_after - x_position_before) / self.dt\n",
        "\n",
        "    # Rewards\n",
        "    ctrl_cost = self.ctrl_cost_weight * np.sum(np.square(action))\n",
        "    forward_reward = self.forward_reward_weight * x_velocity\n",
        "    observation = self.get_obs()\n",
        "    reward = forward_reward - ctrl_cost\n",
        "    terminated = False\n",
        "    info = {\n",
        "        \"x_position\": x_position_after,\n",
        "        \"x_velocity\": x_velocity,\n",
        "        \"reward_run\": forward_reward,\n",
        "        \"reward_ctrl\": -ctrl_cost,\n",
        "    }\n",
        "    return observation, reward, terminated, info\n",
        "\n",
        "  def get_obs(self):\n",
        "    position = self.data.qpos.flat.copy()\n",
        "    velocity = self.data.qvel.flat.copy()\n",
        "    position = position[1:]\n",
        "\n",
        "    observation = np.concatenate((position, velocity)).ravel()\n",
        "    return observation\n",
        "\n",
        "  def render(self):\n",
        "    return self.renderer.render()\n",
        "\n",
        "  def reset(self):\n",
        "    self.reset_simulation()\n",
        "    noise_low = -self.reset_noise_scale\n",
        "    noise_high = self.reset_noise_scale\n",
        "    qpos = self.init_qpos + np.random.uniform(\n",
        "        low=noise_low, high=noise_high, size=self.model.nq\n",
        "    )\n",
        "    qvel = (\n",
        "        self.init_qvel\n",
        "        + self.reset_noise_scale * np.random.standard_normal(self.model.nv)\n",
        "    )\n",
        "    self.set_state(qpos, qvel)\n",
        "    observation = self.get_obs()\n",
        "    return observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b8_buWX0mt6",
        "outputId": "1909c513-2190-40de-9594-58660cdedd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to : cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2NbTAC80mmo"
      },
      "source": [
        "# Soft Actor Critic\n",
        "\n",
        "### Buffer 클래스 정의\n",
        "\n",
        "- 저장 해야 할 것들\n",
        "  - state, $S_{t}$\n",
        "  - next state, $S_{t+1}$\n",
        "  - action, $A_{t}$\n",
        "  - rewards, $R_{t+1}$\n",
        "  - done (terminal information), $d_{t+1}$\n",
        "\n",
        "- Off-policy 알고리즘이므로 정해진 buffer size 만큼 모든 데이터를 수집 가능."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6SKbEXg8opW"
      },
      "outputs": [],
      "source": [
        "# Util 함수들\n",
        "def combined_shape(length, shape=None):\n",
        "    if shape is None:\n",
        "        return (length,)\n",
        "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "def count_vars(module):\n",
        "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud0sNahF0wTG"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    A simple FIFO experience replay buffer for SAC agents.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, size):\n",
        "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "        self.obs2_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
        "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
        "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.idx, self.size, self.max_size = 0, 0, size\n",
        "\n",
        "    def store(self, obs, act, rew, next_obs, done):\n",
        "        self.obs_buf[self.idx] = obs\n",
        "        self.obs2_buf[self.idx] = next_obs\n",
        "        self.act_buf[self.idx] = act\n",
        "        self.rew_buf[self.idx] = rew\n",
        "        self.done_buf[self.idx] = done\n",
        "        self.idx = (self.idx+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def sample_batch(self, batch_size=32):\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        batch = dict(obs=self.obs_buf[idxs],\n",
        "                     obs2=self.obs2_buf[idxs],\n",
        "                     act=self.act_buf[idxs],\n",
        "                     rew=self.rew_buf[idxs],\n",
        "                     done=self.done_buf[idxs])\n",
        "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76TjPn9ORitC"
      },
      "source": [
        "### Policy Network and Q Network\n",
        "\n",
        "- Policy Network: Observation이 입력, 평균 $\\mu_{t}$, 표준편차 $\\Sigma_{t}$를 출력하는 네트워크 설계. 정책 함수의 분포로 Squashed Gaussian 분포를 제안하여 사용함\n",
        "  - Squashed Gaussian Distribution\n",
        "  - $u_{t}\\sim\\mathcal{N}(\\mu_{t},\\Sigma_{t})$\n",
        "  - $a_{t}=$tanh$(u_{t})$\n",
        "  - $\\pi(a_{t}|s_{t})=\\mathcal{N}(u_{t}|\\mu_{t},\\Sigma_{t})\\big|$det$\\left(\\frac{da}{du}\\right)\\big|^{-1}$\n",
        "  - $\\big|$det$\\left(\\frac{da}{du}\\right)\\big|^{-1}=\\prod_{i=1}^{d}(1-$tanh$(u_{i})^{2})^{-1}$\n",
        "- Q Network: Observation과 Action이 입력, 1차원의 Q(s,a) 값을 출력하는 네트워크 설계"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGCrUppPRkRn"
      },
      "outputs": [],
      "source": [
        "def mlp(sizes, activation, output_activation=nn.Identity):\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "LOG_STD_MAX = 2\n",
        "LOG_STD_MIN = -20\n",
        "\n",
        "class SquashedGaussianMLPActor(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):\n",
        "        super().__init__()\n",
        "        self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)\n",
        "        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
        "        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)\n",
        "        self.act_limit = act_limit\n",
        "\n",
        "    def forward(self, obs, deterministic=False, with_logprob=True):\n",
        "        net_out = self.net(obs)\n",
        "        mu = self.mu_layer(net_out)\n",
        "        log_std = self.log_std_layer(net_out)\n",
        "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
        "        std = torch.exp(log_std)\n",
        "\n",
        "        # Pre-squash distribution and sample\n",
        "        pi_distribution = Normal(mu, std)\n",
        "        if deterministic:\n",
        "            # Only used for evaluating policy at test time.\n",
        "            pi_action = mu\n",
        "        else:\n",
        "            pi_action = pi_distribution.rsample()\n",
        "\n",
        "        if with_logprob:\n",
        "            logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
        "            logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)\n",
        "        else:\n",
        "            logp_pi = None\n",
        "\n",
        "        pi_action = torch.tanh(pi_action)\n",
        "        pi_action = self.act_limit * pi_action\n",
        "\n",
        "        return pi_action, logp_pi\n",
        "\n",
        "\n",
        "class MLPQFunction(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)\n",
        "\n",
        "    def forward(self, obs, act):\n",
        "        q = self.q(torch.cat([obs, act], dim=-1))\n",
        "        return torch.squeeze(q, -1) # Critical to ensure q has right shape.\n",
        "\n",
        "class MLPActorCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, observation_dim, action_dim, action_limit, hidden_sizes=(256,256),\n",
        "                 activation=nn.ReLU):\n",
        "        super().__init__()\n",
        "\n",
        "        obs_dim = observation_dim\n",
        "        act_dim = action_dim\n",
        "        act_limit = action_limit\n",
        "\n",
        "        # build policy and value functions\n",
        "        self.pi = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)\n",
        "        self.q1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
        "        self.q2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)\n",
        "\n",
        "    def act(self, obs, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            a, _ = self.pi(obs, deterministic, False)\n",
        "            return a.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkTjuVPlHyqD"
      },
      "source": [
        "Environment 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_4kTcyMDcpA"
      },
      "outputs": [],
      "source": [
        "env = HalfCheetahEnv()\n",
        "obs_dim = env.observation_dim\n",
        "act_dim = env.action_dim\n",
        "act_limit = env.action_limit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSmMbucNH3b2"
      },
      "source": [
        "Policy 네트워크 Q1, Q2 네트워크 정의 및 각 네트워크별 optimizer 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "497cH6mIDcmD",
        "outputId": "b2db1ecc-6178-4d64-bb04-72f7c80fc48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of parameters: \t pi: 73484, \t q1: 72193, \t q2: 72193\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lr=1e-3\n",
        "\n",
        "# Create actor-critic module and target networks\n",
        "ac = MLPActorCritic(obs_dim, act_dim, act_limit)\n",
        "ac_targ = deepcopy(ac)\n",
        "\n",
        "# List of parameters for both Q-networks (save this for convenience)\n",
        "q_params = list(ac.q1.parameters()) + list(ac.q2.parameters())\n",
        "\n",
        "# Count variables (protip: try to get a feel for how different size networks behave!)\n",
        "var_counts = tuple(count_vars(module) for module in [ac.pi, ac.q1, ac.q2])\n",
        "print('\\nNumber of parameters: \\t pi: %d, \\t q1: %d, \\t q2: %d\\n'%var_counts)\n",
        "\n",
        "# Set up optimizers for policy and q-function\n",
        "pi_optimizer = Adam(ac.pi.parameters(), lr=lr)\n",
        "q_optimizer = Adam(q_params, lr=lr)\n",
        "\n",
        "# Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
        "for p in ac_targ.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHAcl1aEH_3K"
      },
      "source": [
        "Replay buffer 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-28PPOSkDci6"
      },
      "outputs": [],
      "source": [
        "replay_size=200000\n",
        "\n",
        "# Experience buffer\n",
        "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfQhGfVtIDY2"
      },
      "source": [
        "에피소드 수집 및 buffer에 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoO9jFbcDcfy"
      },
      "outputs": [],
      "source": [
        "total_steps = 1000\n",
        "max_ep_len = 1000\n",
        "\n",
        "o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "# Main loop: collect experience in env and update/log each epoch\n",
        "for t in range(total_steps):\n",
        "\n",
        "    a = env.sample_action()\n",
        "\n",
        "    # Step the env\n",
        "    o2, r, d, _ = env.step(a)\n",
        "    ep_ret += r\n",
        "    ep_len += 1\n",
        "\n",
        "    # Ignore the \"done\" signal if it comes from hitting the time\n",
        "    # horizon (that is, when it's an artificial terminal signal\n",
        "    # that isn't based on the agent's state)\n",
        "    d = False if ep_len==max_ep_len else d\n",
        "\n",
        "    # Store experience to replay buffer\n",
        "    replay_buffer.store(o, a, r, o2, d)\n",
        "\n",
        "    # Super critical, easy to overlook step: make sure to update\n",
        "    # most recent observation!\n",
        "    o = o2\n",
        "\n",
        "    # End of trajectory handling\n",
        "    if d or (ep_len == max_ep_len):\n",
        "        o, ep_ret, ep_len = env.reset(), 0, 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2J5cn2BIHaW"
      },
      "source": [
        "Buffer에서 batch sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5o-sgP_Dcbo"
      },
      "outputs": [],
      "source": [
        "batch_size=256\n",
        "batch = replay_buffer.sample_batch(batch_size)\n",
        "o, a, r, o2, d = batch['obs'], batch['act'], batch['rew'], batch['obs2'], batch['done']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aCOAQQSIKjI"
      },
      "source": [
        "Q function loss 계산 및 gradient step 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGNOYyQNDcTg"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "alpha = 0.2\n",
        "\n",
        "q1 = ac.q1(o,a)\n",
        "q2 = ac.q2(o,a)\n",
        "\n",
        "# Bellman backup for Q functions\n",
        "with torch.no_grad():\n",
        "    # Target actions come from *current* policy\n",
        "    a2, logp_a2 = ac.pi(o2)\n",
        "\n",
        "    # Target Q-values\n",
        "    q1_pi_targ = ac_targ.q1(o2, a2)\n",
        "    q2_pi_targ = ac_targ.q2(o2, a2)\n",
        "    q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
        "    backup = r + gamma * (1 - d) * (q_pi_targ - alpha * logp_a2)\n",
        "\n",
        "# MSE loss against Bellman backup\n",
        "loss_q1 = ((q1 - backup)**2).mean()\n",
        "loss_q2 = ((q2 - backup)**2).mean()\n",
        "loss_q = loss_q1 + loss_q2\n",
        "\n",
        "# First run one gradient descent step for Q1 and Q2\n",
        "q_optimizer.zero_grad()\n",
        "loss_q.backward()\n",
        "q_optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce80kTxbIOYP"
      },
      "source": [
        "Policy loss 계산 및 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5Se1MboDcQo"
      },
      "outputs": [],
      "source": [
        "a, logp_pi = ac.pi(o)\n",
        "q1_pi = ac.q1(o, a)\n",
        "q2_pi = ac.q2(o, a)\n",
        "q_pi = torch.min(q1_pi, q2_pi)\n",
        "\n",
        "# Entropy-regularized policy loss\n",
        "loss_pi = (alpha * logp_pi - q_pi).mean()\n",
        "\n",
        "# Next run one gradient descent step for pi.\n",
        "pi_optimizer.zero_grad()\n",
        "loss_pi.backward()\n",
        "pi_optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci2zKfUTISk2"
      },
      "source": [
        "Target network에 soft update 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0iuhtSkHXug"
      },
      "outputs": [],
      "source": [
        "polyak = 0.995\n",
        "# Finally, update target networks by polyak averaging.\n",
        "with torch.no_grad():\n",
        "    for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
        "        p_targ.data.mul_(polyak)\n",
        "        p_targ.data.add_((1 - polyak) * p.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZluq_H1mz61"
      },
      "source": [
        "### Soft Actor Critic\n",
        "- 앞서 만든 클래스들과 함수들을 모두 합쳐 SAC 알고리즘 구현\n",
        "- Q Loss:\n",
        "  - Target 값 계산: $y_t=r_{t+1}+\\gamma (1-d_{t+1}) \\left(\\min_{j\\in(1,2)}Q_{\\phi_{j}}(s_{t+1},\\tilde{a}) - \\alpha \\log \\pi_{\\theta}(\\tilde{a}|s_{t+1})\\right)$\n",
        "    - $\\tilde{a}\\sim\\pi_{\\theta}(\\cdot|s_{t+1})$: Next state에 대한 action은 policy network에서 sampling 하여 사용하는 것이 핵심.\n",
        "  - MSE Loss: $\\sum_{i=1}^{B}\\left(Q_{j}(s_{t_i},a_{t_i}) - y_{t_i}\\right)^{2}$\n",
        "  - 실제 논문의 구현과 차이가 있음.\n",
        "- Policy Loss:\n",
        "  - Maximize value: $- \\sum_{i=1}^{B}Q_{\\pi}(s_{t_i},\\tilde{a}) - \\alpha \\log \\pi_{\\theta}(\\tilde{a}|s_{t_i})$\n",
        "      - $\\tilde{a}\\sim\\pi_{\\theta}(\\cdot|s_{t+1})$: Next state에 대한 action은 policy network에서 sampling 하여 사용하는 것이 핵심.\n",
        "      - $Q_{\\pi}(s_{t_i},\\tilde{a}) = \\min_{j\\in(1,2)}Q_{\\phi_{j}}(s_{t_i},\\tilde{a})$: 보수적인 Q Value를 사용하는 것이 Overestimation을 방지할 수 있음.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yinJNyzARkxt"
      },
      "outputs": [],
      "source": [
        "def sac(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0,\n",
        "        steps_per_epoch=5000, epochs=5, replay_size=int(1e6), gamma=0.99,\n",
        "        polyak=0.995, lr=1e-3, alpha=0.2, batch_size=100, start_steps=10000,\n",
        "        update_after=1000, update_every=50, num_test_episodes=40, max_ep_len=1000):\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    EpRet = []\n",
        "    TestEpRet = []\n",
        "    EpLen = []\n",
        "    TestEpLen = []\n",
        "    TotalEnvInteracts = []\n",
        "    Q1Vals = []\n",
        "    Q2Vals = []\n",
        "    LogPi = []\n",
        "    LossPi = []\n",
        "    LossQ = []\n",
        "    Time = []\n",
        "\n",
        "    env, test_env = env_fn(), env_fn()\n",
        "    obs_dim = env.observation_dim\n",
        "    act_dim = env.action_dim\n",
        "    act_limit = env.action_limit\n",
        "\n",
        "    # Create actor-critic module and target networks\n",
        "    ac = actor_critic(obs_dim, act_dim, act_limit, **ac_kwargs)\n",
        "    ac_targ = deepcopy(ac)\n",
        "\n",
        "    # List of parameters for both Q-networks (save this for convenience)\n",
        "    q_params = list(ac.q1.parameters()) + list(ac.q2.parameters())\n",
        "\n",
        "    # Count variables (protip: try to get a feel for how different size networks behave!)\n",
        "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.q1, ac.q2])\n",
        "    print('\\nNumber of parameters: \\t pi: %d, \\t q1: %d, \\t q2: %d\\n'%var_counts)\n",
        "\n",
        "    # Set up optimizers for policy and q-function\n",
        "    pi_optimizer = Adam(ac.pi.parameters(), lr=lr)\n",
        "    q_optimizer = Adam(q_params, lr=lr)\n",
        "\n",
        "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
        "    for p in ac_targ.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Experience buffer\n",
        "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
        "\n",
        "    # Set up function for computing SAC Q-losses\n",
        "    def compute_loss_q(data):\n",
        "        o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
        "\n",
        "        q1 = ac.q1(o,a)\n",
        "        q2 = ac.q2(o,a)\n",
        "\n",
        "        # Bellman backup for Q functions\n",
        "        with torch.no_grad():\n",
        "            # Target actions come from *current* policy\n",
        "            a2, logp_a2 = ac.pi(o2)\n",
        "\n",
        "            # Target Q-values\n",
        "            q1_pi_targ = ac_targ.q1(o2, a2)\n",
        "            q2_pi_targ = ac_targ.q2(o2, a2)\n",
        "            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)\n",
        "            backup = r + gamma * (1 - d) * (q_pi_targ - alpha * logp_a2)\n",
        "\n",
        "        # MSE loss against Bellman backup\n",
        "        loss_q1 = ((q1 - backup)**2).mean()\n",
        "        loss_q2 = ((q2 - backup)**2).mean()\n",
        "        loss_q = loss_q1 + loss_q2\n",
        "\n",
        "        # Useful info for logging\n",
        "        q_info = dict(Q1Vals=q1.detach().numpy(),\n",
        "                      Q2Vals=q2.detach().numpy())\n",
        "\n",
        "        return loss_q, q_info\n",
        "\n",
        "    # Set up function for computing SAC pi loss\n",
        "    def compute_loss_pi(data):\n",
        "        o = data['obs']\n",
        "        a, logp_pi = ac.pi(o)\n",
        "        q1_pi = ac.q1(o, a)\n",
        "        q2_pi = ac.q2(o, a)\n",
        "        q_pi = torch.min(q1_pi, q2_pi)\n",
        "\n",
        "        # Entropy-regularized policy loss\n",
        "        loss_pi = (alpha * logp_pi - q_pi).mean()\n",
        "\n",
        "        # Useful info for logging\n",
        "        pi_info = dict(LogPi=logp_pi.detach().numpy())\n",
        "\n",
        "        return loss_pi, pi_info\n",
        "\n",
        "    def update(data):\n",
        "        # First run one gradient descent step for Q1 and Q2\n",
        "        q_optimizer.zero_grad()\n",
        "        loss_q, q_info = compute_loss_q(data)\n",
        "        loss_q.backward()\n",
        "        q_optimizer.step()\n",
        "\n",
        "        # Record things\n",
        "        LossQ.append(loss_q.item())\n",
        "        Q1Vals.append(q_info['Q1Vals'])\n",
        "        Q2Vals.append(q_info['Q2Vals'])\n",
        "\n",
        "        # Freeze Q-networks so you don't waste computational effort\n",
        "        # computing gradients for them during the policy learning step.\n",
        "        for p in q_params:\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Next run one gradient descent step for pi.\n",
        "        pi_optimizer.zero_grad()\n",
        "        loss_pi, pi_info = compute_loss_pi(data)\n",
        "        loss_pi.backward()\n",
        "        pi_optimizer.step()\n",
        "\n",
        "        # Unfreeze Q-networks so you can optimize it at next DDPG step.\n",
        "        for p in q_params:\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # Record things\n",
        "        LossPi.append(loss_pi.item())\n",
        "        LogPi.append(pi_info['LogPi'])\n",
        "\n",
        "        # Finally, update target networks by polyak averaging.\n",
        "        with torch.no_grad():\n",
        "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
        "                # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
        "                # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
        "                p_targ.data.mul_(polyak)\n",
        "                p_targ.data.add_((1 - polyak) * p.data)\n",
        "\n",
        "    def get_action(o, deterministic=False):\n",
        "        return ac.act(torch.as_tensor(o, dtype=torch.float32),\n",
        "                      deterministic)\n",
        "\n",
        "    def test_agent():\n",
        "        for j in range(num_test_episodes):\n",
        "            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
        "            while not(d or (ep_len == max_ep_len)):\n",
        "                # Take deterministic actions at test time\n",
        "                o, r, d, _ = test_env.step(get_action(o, True))\n",
        "                ep_ret += r\n",
        "                ep_len += 1\n",
        "            TestEpRet.append(ep_ret)\n",
        "            TestEpLen.append(ep_len)\n",
        "\n",
        "    # Prepare for interaction with environment\n",
        "    total_steps = steps_per_epoch * epochs\n",
        "    start_time = time.time()\n",
        "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "    # Main loop: collect experience in env and update/log each epoch\n",
        "    for t in range(total_steps):\n",
        "\n",
        "        # Until start_steps have elapsed, randomly sample actions\n",
        "        # from a uniform distribution for better exploration. Afterwards,\n",
        "        # use the learned policy.\n",
        "        if t > start_steps:\n",
        "            a = get_action(o)\n",
        "        else:\n",
        "            a = env.sample_action()\n",
        "\n",
        "        # Step the env\n",
        "        o2, r, d, _ = env.step(a)\n",
        "        ep_ret += r\n",
        "        ep_len += 1\n",
        "\n",
        "        # Ignore the \"done\" signal if it comes from hitting the time\n",
        "        # horizon (that is, when it's an artificial terminal signal\n",
        "        # that isn't based on the agent's state)\n",
        "        d = False if ep_len==max_ep_len else d\n",
        "\n",
        "        # Store experience to replay buffer\n",
        "        replay_buffer.store(o, a, r, o2, d)\n",
        "\n",
        "        # Super critical, easy to overlook step: make sure to update\n",
        "        # most recent observation!\n",
        "        o = o2\n",
        "\n",
        "        # End of trajectory handling\n",
        "        if d or (ep_len == max_ep_len):\n",
        "            EpRet.append(ep_ret)\n",
        "            EpLen.append(ep_len)\n",
        "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "\n",
        "        # Update handling\n",
        "        if t >= update_after and t % update_every == 0:\n",
        "            for j in range(update_every):\n",
        "                batch = replay_buffer.sample_batch(batch_size)\n",
        "                update(data=batch)\n",
        "\n",
        "        # End of epoch handling\n",
        "        if (t+1) % steps_per_epoch == 0:\n",
        "            epoch = (t+1) // steps_per_epoch\n",
        "\n",
        "            # Test the performance of the deterministic version of the agent.\n",
        "            test_agent()\n",
        "\n",
        "            TotalEnvInteracts.append(t)\n",
        "            Time.append(time.time()-start_time)\n",
        "            print(f'[Epoch:{epoch}] TestEpRet:{np.min(TestEpRet[-10:]):8.2f} < {np.mean(TestEpRet[-10:]):8.2f} < {np.max(TestEpRet[-10:]):8.2f}, TestEpLen:{np.mean(TestEpLen[-10:]):8.2f}, EpRet:{np.min(EpRet[-10:]):8.2f} < {np.mean(EpRet[-10:]):8.2f} < {np.max(EpRet[-10:]):8.2f}, EpLen:{np.mean(EpLen[-10:]):8.2f}, Q1Vals:{np.mean(Q1Vals[-10:]):8.2f}, Q2Vals:{np.mean(Q2Vals[-10:]):8.2f}, TotalEnvInteracts:{TotalEnvInteracts[-1]:8d}, LossPi:{np.mean(LossPi[-10:]):8.2f}, LossQ:{np.mean(LossQ[-10:]):8.2f}, Time:{Time[-1]:8.2f}')\n",
        "    return ac, EpRet, EpLen, Q1Vals, Q2Vals, TotalEnvInteracts, LossPi, LossQ, Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZdNqQo7RlfP",
        "outputId": "498622cb-597c-4819-91f9-6e0bd6e5df7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of parameters: \t pi: 73484, \t q1: 72193, \t q2: 72193\n",
            "\n",
            "[Epoch:1] TestEpRet:  140.53 <   302.88 <   349.67, TestEpLen: 1000.00, EpRet: -408.55 <  -302.33 <  -229.21, EpLen: 1000.00, Q1Vals:    9.69, Q2Vals:    9.67, TotalEnvInteracts:    4999, LossPi:  -10.71, LossQ:    0.98, Time:   93.69\n",
            "[Epoch:2] TestEpRet: -160.29 <     8.75 <    61.12, TestEpLen: 1000.00, EpRet: -454.36 <  -323.10 <  -229.21, EpLen: 1000.00, Q1Vals:   34.96, Q2Vals:   34.95, TotalEnvInteracts:    9999, LossPi:  -36.67, LossQ:    3.49, Time:  198.58\n",
            "[Epoch:3] TestEpRet:  661.19 <   907.47 <  1004.76, TestEpLen: 1000.00, EpRet: -454.36 <  -198.35 <    93.83, EpLen: 1000.00, Q1Vals:   37.75, Q2Vals:   37.71, TotalEnvInteracts:   14999, LossPi:  -38.42, LossQ:    2.74, Time:  300.90\n",
            "[Epoch:4] TestEpRet: 1470.78 <  1523.01 <  1572.72, TestEpLen: 1000.00, EpRet: -303.67 <   210.28 <   837.50, EpLen: 1000.00, Q1Vals:   43.00, Q2Vals:   43.01, TotalEnvInteracts:   19999, LossPi:  -43.58, LossQ:    3.19, Time:  403.33\n",
            "[Epoch:5] TestEpRet: 2299.85 <  2361.29 <  2415.73, TestEpLen: 1000.00, EpRet:  -40.70 <   841.58 <  1528.72, EpLen: 1000.00, Q1Vals:   52.24, Q2Vals:   52.27, TotalEnvInteracts:   24999, LossPi:  -52.91, LossQ:    3.27, Time:  503.52\n",
            "[Epoch:6] TestEpRet: 2967.67 <  3041.38 <  3165.90, TestEpLen: 1000.00, EpRet:  861.27 <  1702.92 <  2469.76, EpLen: 1000.00, Q1Vals:   72.17, Q2Vals:   72.19, TotalEnvInteracts:   29999, LossPi:  -72.83, LossQ:    5.39, Time:  604.96\n",
            "[Epoch:7] TestEpRet: 3193.09 <  3359.69 <  3520.43, TestEpLen: 1000.00, EpRet: 1948.47 <  2486.59 <  2972.15, EpLen: 1000.00, Q1Vals:   91.42, Q2Vals:   91.34, TotalEnvInteracts:   34999, LossPi:  -92.16, LossQ:    7.05, Time:  708.23\n",
            "[Epoch:8] TestEpRet: 3503.42 <  3762.99 <  4019.96, TestEpLen: 1000.00, EpRet: 2585.75 <  3080.50 <  3646.19, EpLen: 1000.00, Q1Vals:  119.19, Q2Vals:  119.16, TotalEnvInteracts:   39999, LossPi: -119.97, LossQ:    8.65, Time:  810.93\n",
            "[Epoch:9] TestEpRet: 4131.02 <  4271.92 <  4373.04, TestEpLen: 1000.00, EpRet: 3154.10 <  3572.95 <  3984.24, EpLen: 1000.00, Q1Vals:  148.37, Q2Vals:  148.42, TotalEnvInteracts:   44999, LossPi: -149.08, LossQ:   11.09, Time:  910.41\n",
            "[Epoch:10] TestEpRet:  648.32 <  4285.02 <  4840.91, TestEpLen: 1000.00, EpRet: 3506.53 <  3964.09 <  4292.90, EpLen: 1000.00, Q1Vals:  175.57, Q2Vals:  175.57, TotalEnvInteracts:   49999, LossPi: -176.25, LossQ:   10.80, Time: 1013.74\n"
          ]
        }
      ],
      "source": [
        "sac = sac(lambda : HalfCheetahEnv(), actor_critic=MLPActorCritic,\n",
        "          ac_kwargs=dict(hidden_sizes=[256, 256]),\n",
        "          gamma=0.99, seed=0, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOJLSpp1suPk"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "7hzb3nbzR4I5",
        "outputId": "c2511bfd-d312-44ca-afae-22bce83cf794"
      },
      "outputs": [],
      "source": [
        "env = HalfCheetahEnv()\n",
        "imgs = []\n",
        "\n",
        "obs = env.reset()\n",
        "for t in range(1000):\n",
        "  with torch.no_grad():\n",
        "    obs = torch.as_tensor(obs, dtype=torch.float32)\n",
        "    action = sac[0].act(obs, True)\n",
        "  obs, reward, terminated, info = env.step(action)\n",
        "  img = env.render()\n",
        "  imgs.append(img)\n",
        "\n",
        "media.show_video(imgs, fps=1/env.dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kemTuCzrRw7e"
      },
      "outputs": [],
      "source": [
        "num_trajs = 100\n",
        "max_ep_len = 1000\n",
        "\n",
        "actions = []\n",
        "observations = []\n",
        "next_observations = []\n",
        "rewards = []\n",
        "terminals = []\n",
        "ravg = []\n",
        "\n",
        "o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "for t in range(num_trajs*max_ep_len):\n",
        "  with torch.no_grad():\n",
        "    a = sac[0].act(torch.as_tensor(o, dtype=torch.float32), True)\n",
        "\n",
        "  o2, r, d, _ = env.step(a) # 시뮬레이션 진행\n",
        "  ep_ret += r\n",
        "  ep_len += 1\n",
        "  d = False if ep_len==max_ep_len else d\n",
        "\n",
        "  # 데이터 저장\n",
        "  observations.append(o)\n",
        "  actions.append(a)\n",
        "  next_observations.append(o2)\n",
        "  rewards.append(r)\n",
        "  terminals.append(d)\n",
        "\n",
        "  # Oservation 업데이트\n",
        "  o = o2\n",
        "  if d or (ep_len == max_ep_len):\n",
        "      ravg.append(ep_ret)\n",
        "      o, ep_ret, ep_len = env.reset(), 0, 0\n",
        "print(f'오프라인 데이터 셋의 평균 리턴 : {np.mean(ravg)}')\n",
        "\n",
        "# Type 정리\n",
        "observations = np.array(observations).astype(np.float32)\n",
        "actions = np.array(actions).astype(np.float32)\n",
        "next_observations = np.array(next_observations).astype(np.float32)\n",
        "rewards = np.array(rewards).astype(np.float32)\n",
        "terminals = np.array(terminals).astype(np.bool_)\n",
        "\n",
        "# Dictionary로 만들어서 Replay Buffer 생성\n",
        "dataset = {\"observations\":observations,\"actions\":actions,\"next_observations\":next_observations,\"rewards\":rewards,\"terminals\":terminals,\"ravg\":ravg}\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('expert_dataset.pickle','wb') as f:\n",
        "    pickle.dump(dataset, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}